[[appendix_project_structure]]
[appendix]
== A Template Project Structure

Around <<chapter_04_service_layer>> we moved from just having
everything in one folder to a more structured tree, and we thought it might
be of interest to outline the moving parts.

[TIP]
====
You can find our code for this chapter at
https://github.com/cosmicpython/code/tree/appendix_project_structure[github.com/cosmicpython/code/tree/appendix_project_structure].

----
git clone https://github.com/cosmicpython/code.git cosmic-code && cd cosmic-code
git checkout appendix_project_structure
----
====


<<project_tree>> shows the folder structure:

[[project_tree]]
.Project tree
====
[source,text]
[role="tree"]
----
.
├── docker-compose.yml  <1>
├── Dockerfile  <1>
├── license.txt
├── Makefile  <2>
├── mypy.ini
├── README.md
├── requirements.txt
├── src  <3>
│   ├── allocation
│   │   ├── config.py
│   │   ├── flask_app.py
│   │   ├── model.py
│   │   ├── orm.py
│   │   ├── repository.py
│   │   └── services.py
│   └── setup.py  <3>
└── tests  <4>
    ├── conftest.py  <4>
    ├── e2e
    │   └── test_api.py
    ├── integration
    │   ├── test_orm.py
    │   └── test_repository.py
    ├── pytest.ini  <4>
    └── unit
        ├── test_allocate.py
        ├── test_batches.py
        └── test_services.py
----
====

<1> Our _docker-compose.yml_ and our _Dockerfile_ are the main bits of configuration
    for the containers that run our app, and can also run the tests (for CI).  A
    more complex project might have several Dockerfiles, although we've found that
    minimising the number of images is usually a good idea.footnote:[Splitting
    out images for prod and test is sometimes a good idea, but we've tended
    to find that going further and trying to split out different images for
    different types of application code (eg web api vs pubsub client) usually
    ends up being more trouble than it's worth; the cost in terms of complexity
    and longer rebuild/CI times is too high. YMMV.]

<2> A __Makefile__footnote:[http://www.pyinvoke.org/[Invoke] is a pure-Python alternative to Makefiles,
    worth checking out if everyone in your team knows Python (or at least knows
    it better than Bash!)]
    provides the entrypoint for all the typical commands a developer
    (or a CI server) might want to run during their normal workflow.  `make
    build`, `make test`, and so on.  This is optional, you could just use
    `docker-compose` and `pytest` directly, but if nothing else it's nice to
    have all the "common commands" in a list somewhere, and unlike
    documentation, a Makefile is code so it has less tendency to go out of date.

<3> All the actual source code for our app, including the domain model, the
    flask app, and infrastructure code, lives in a Python package inside
    _src_,footnote:[More on _src_ folders: https://hynek.me/articles/testing-packaging/]
    which we install using `pip install -e` and the _setup.py_ file.  This makes
    imports easy. Currently the structure within this module is totally flat,
    but for a more complex project you'd expect to grow a folder hierarchy
    including _domain_model/_, _infrastructure/_, _services/_, _api/_


<4> Tests live in their own folder, with subfolders to distinguish different test
    types, and allow you to run them separately.  We can keep shared fixtures
    (_conftest.py_) in the main tests folder, and nest more specific ones if we
    wish. This is also the place to keep _pytest.ini_.



TIP:  The https://docs.pytest.org/en/latest/goodpractices.html#choosing-a-test-layout-import-rules[pytest docs]
    are really good on test layout and importability.



Let's look at a few of these in more detail.

////
TODO (DS): All this seems sensible. It would be nice to include a dependency
graph so we can see the layering within src/allocation.

Maybe should include message bus too?
////



=== Env Vars, 12-Factor, and Config, Inside and Outside Containers.

The basic problem we're trying to solve here is that we need different
config settings for:

- Running code or tests directly from your own dev machine, perhaps
  talking to mapped ports from docker containers

- Running on the containers themselves, with "real" ports and hostnames

- And different settings for different container environments, dev,
  staging, prod, and so on.

// TODO (DS): Not totally clear on the specifics of what you're saying in these
// bullet points, though of course i understand in general.

Configuration through environment variables as suggested by the
https://12factor.net/config[12-factor] manifesto will solve this problem,
but concretely, how do we implement it in our code and our containers?


=== Config.py

////
TODO:
Ed:

Would you consider this a singleton?

I have some past negative experiences with this style of configuration, because
it can be easily abused. The env var mitigates against that, and I suppose this
varies from codebase to codebase.

Bob:
Not strictly. It's possible to create more than one of them, but it's unlikely
that I'd do so outside of unit tests. I more or less think of these config
classes as part of my composition root. They tend only to be used by the entry
point to the application.

Ed:
"Entry point to the application" is key, I think. The anti-pattern I've seen is
where the config just gets imported anywhere, and anything remotely related to
configuration gets put in there.

https://github.com/cosmicpython/book/issues/52
////
// TODO (DS): I reckon configuration patterns are an important part of the architecture your outlining, i wonder if they belong in the main book?

Whenever our application code needs access to some config, it's going to
get it from a file called _config.py__. <<config_dot_py>> shows a couple of
examples from our app:

[[config_dot_py]]
.Sample config functions (src/allocation/config.py)
====
[source,python]
----
import os

def get_postgres_uri():  #<1>
    host = os.environ.get('DB_HOST', 'localhost')  #<2>
    port = 54321 if host == 'localhost' else 5432
    password = os.environ.get('DB_PASSWORD', 'abc123')
    user, db_name = 'allocation', 'allocation'
    return f"postgresql://{user}:{password}@{host}:{port}/{db_name}"


def get_api_url():
    host = os.environ.get('API_HOST', 'localhost')
    port = 5005 if host == 'localhost' else 80
    return f"http://{host}:{port}"
----
====

<1> We use functions for getting the current config, rather than constants
    available at import time, because that allows client code to modify
    `os.environ` if it needs to.

<2> _config.py_ also defines some default settings, designed to work when
    running the code from the developer's local machinefootnote:[You might prefer
    to fail hard if an env var is not set, but this gives us a local dev
    setup that "just works" (as much as possible).].

// TODO (DS): The way config interacts with dependency injection might be worth
// a diagram (ie the layers)

// TODO (DS): Say something about mutability of config here? I tend to think
// it's good for it to be immutable in runtime environments, but mutable in
// tests... Not sure though

=== Docker-Compose and Containers Config

We use a lightweight docker container orchestration tool called docker-compose.
It's main configuration is via a YAML file (sighfootnote:[Harry hates YAML. He says
he can never remember the syntax or how it's supposed to indent.]),
<<docker_compose>>:


[[docker_compose]]
.Docker-Compose config file (docker-compose.yml)
====
[source,yaml]
----
version: "3"
services:

  app:  #<1>
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      - postgres
    environment:  #<3>
      - DB_HOST=postgres  <4>
      - DB_PASSWORD=abc123
      - API_HOST=app
      - PYTHONDONTWRITEBYTECODE=1  #<5>
    volumes:  #<6>
      - ./src:/src
      - ./tests:/tests
    ports:
      - "5005:80"  <7>


  postgres:
    image: postgres:9.6  #<2>
    environment:
      - POSTGRES_USER=allocation
      - POSTGRES_PASSWORD=abc123
    ports:
      - "54321:5432"
----
====

<1> In the docker-compose file, we define the different "services"
    (containers) that we need for our app.  Usually one main image
    contains all our code, and we can use it to run our API, our tests,
    or any other service that needs access to the domain model.

<2> You'll probably have some other infrastructure services like a database.
    In production you may not use containers for this, you might have a cloud
    provider instead, but _docker-compose_ gives us a way of producing a
    similar service for dev or CI.

<3> The `environment` stanza lets you set the environment variables for your
    containers, the hostnames and ports as seen from inside the docker cluster.
    If you have enough containers that information starts to be duplicated in
    these sections, you can use `environment_file` instead.  We usually call
    ours _container.env_.

<4> Inside a cluster, docker-compose sets up networking such that containers are
    available to each other via hostnames named after their service name.

<5> Protip: if you're mounting volumes to share source folders between your
    local dev machine and the container, the `PYTHONDONTWRITEBYTECODE` env
    var tells Python to not write `.pyc` files, and that will save you from
    having millions of root-owned files sprinkled all over your local filesystem,
    being all annoying to delete, and causing weird python compiler errors besides.

<6> Mounting our source and test code as `volumes` means we don't need to rebuild
    our containers every time we make a code change.

<7> And the `ports` section allows us to expose the ports from inside the containers
    to the outside worldfootnote:[On a CI server you may not be able to expose
    arbitrary ports reliably, but it's only a convenience for local dev. You
    can find ways of making these port mappings optional, eg with
    docker-compose.override.yml]--these correspond to the default ports we set
    in _config.py_.

NOTE: Inside docker, other containers are available through hostnames named after
    their service name. Outside docker, they are available on `localhost`, at the
    port defined in the `ports` section.


=== Installing Your Source as a Package

All our application code (everything except tests really) lives inside an
_src_ folder, as in <<src_folder_tree>>:

[[src_folder_tree]]
.The src folder
====
[source,text]
[role="skip"]
----
├── src
│   ├── allocation  #<1>
│   │   ├── config.py
│   │   └── ...
│   └── setup.py  <2>
----
====

<1> Subfolders define top-level module names.  You can have multiple if you like.
<2> And _setup.py_ is the file you need to make it pip-installable.  See
    <<setup_dot_py>>.

[[setup_dot_py]]
.pip-installable modules in 3 lines  (src/setup.py)
====
[source,python]
----
from setuptools import setup

setup(
    name='allocation',
    version='0.1',
    packages=['allocation'],
)
----
====

That's all you need.  `packages=` specifies the names of subfolders that you
want to install as top-level modules. The `name` entry is just cosmetic, but
it's required. For a package that's never actually going to hit PyPI, this is
all you need.

// TODO (DS): Offhand, I think this might fail if you had any subpackages, as
// it won't install those files?

=== More nested folder structures

As an application grows in complexity, you may decide to start putting things
into subfolders according to their role in the architecture.  Here's on naming
convention we've experimented with:

[[more_nested_folders]]
.A place for everything and everything in its place.
====
[source,text]
[role="skip"]
----
src/allocation
├── config.py
├── domain  <1>
│   ├── commands.py
│   ├── events.py
│   └── model.py
├── adapters  <2>
│   ├── email.py
│   ├── orm.py
│   ├── redis_client.py
│   └── repository.py
├── entrypoints  <3>
│   ├── flask_app.py
│   └── redis_eventconsumer.py
└── service_layer <4>
    ├── exceptions.py
    ├── handlers.py
    ├── messagebus.py
    └── unit_of_work.py
----
====

<1> The domain includes your core model classes, as well as events and comments.  You might
    also include some domain-layer exceptions in here.

<2> Adapters are our abstractions that let us interface with permanent storage and other.
    In Ports & Adapters terminology these are _secondary_ adapters or _driven_ adapters,
    or sometimes _inward-facing_ adapters.

<3> Entrypoints are the places that allow commands and information to come into our app.
    These are adapters too, sometimes called primary or driving or outward-facing adapters.

<4> The `service_layer` folder you could also call `orchestration` perhaps, since it's got
    all the gubbins we need for our event-driven architecture, which is the way we actually
    present our use cases to the outside world, and put the core domain model to work.


=== Dockerfile

Dockerfiles are going to be very project-specific, but here's a few key stages
you'll expect to see:

[[dockerfile]]
.Our Dockerfile (Dockerfile)
====
[source,dockerfile]
----
FROM python:3.8-alpine

<1>
RUN apk add --no-cache --virtual .build-deps gcc postgresql-dev musl-dev python3-dev
RUN apk add libpq

<2>
COPY requirements.txt /tmp/
RUN pip install -r /tmp/requirements.txt

RUN apk del --no-cache .build-deps

<3>
RUN mkdir -p /src
COPY src/ /src/
RUN pip install -e /src
COPY tests/ /tests/

<4>
WORKDIR /src
ENV FLASK_APP=allocation/flask_app.py FLASK_DEBUG=1 PYTHONUNBUFFERED=1
CMD flask run --host=0.0.0.0 --port=80
----
====

<1> Installing system-level dependencies
<2> Installing our Python dependencies
<3> Copying and installing our source
<4> Optionally configuring a default startup command (you'll probably override
    this a lot from the command-line)

TIP: One thing to note is that we install things in the order of how frequently they
    are likely to change.  This allows us to maximise docker build cache reuse. I
    can't tell you how much pain and frustration belies this lesson.


=== Tests

Our tests are kept alongside everything else, as in <<tests_folder>>:

[[tests_folder]]
.Tests folder tree
====
[source,text]
[role="tree"]
----
└── tests
    ├── conftest.py
    ├── e2e
    │   └── test_api.py
    ├── integration
    │   ├── test_orm.py
    │   └── test_repository.py
    ├── pytest.ini
    └── unit
        ├── test_allocate.py
        ├── test_batches.py
        └── test_services.py
----
====

Nothing particularly clever here, just some separation of different test types
that you're likely to want to run separately, and some files for common fixtures,
config and so on.

There's no _src_ folder or _setup.py_ in the tests folders because we've not usually
found we need to make tests pip-installable, but if you have difficulties with
import paths, you might find it helps.


=== Wrap-up

Those are our basic building blocks:

* Source code in an _src_ folder, pip-installable using setup.py
* Some docker config for spinning up a local cluster that mirrors production as far as possible
* Configuration via environment variables, centralised in a Python file called _config.py_, with
  defaults allowing things to run _outside_ containers.
* And a Makefile for useful command-line, um, commands.

We doubt that anyone will end up with _exactly_ the same solutions we did, but we hope you
find some inspiration here.
