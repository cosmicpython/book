[[chapter_03_flask_api_and_service_layer]]
== Our first use case:  Flask API and service layer.

Like any good agile team, we're hustling to try and get an MVP out and
in front of the users to start gathering feedback.  We have the core
of our domain model and the domain service we need to allocate orders,
and we have the Repository interface for permanent storage.

Let's try and plug all the moving parts together as quickly as we
can, and then refactor towards a cleaner architecture.


=== A first end-to-end (E2E) test

No-one is interested in getting into a long terminology debate about what
counts as an E2E test vs a functional test vs an acceptance test vs an
integration test vs unit tests.  Different projects need different combinations
of tests, and we've seen perfectly successful projects just split things into
"fast tests" and "slow tests".

For now we want to write one or maybe two tests that are going to exercise
a "real" API endpoint (using HTTP) and talk to a real database. Let's call
them end-to-end tests because it's one of the most self-explanatory names.

<<first_api_test>> shows a first cut:

[[first_api_test]]
.A first API test (test_api.py)
====
[source,python]
[role="non-head"]
----
def random_ref(prefix):
    return prefix + '-' + uuid.uuid4().hex[:10]

@pytest.mark.usefixtures('restart_api')
def test_api_returns_allocation(add_stock):
    sku, othersku = random_ref('s1'), random_ref('s2')
    batch1, batch2, batch3 = random_ref('b1'), random_ref('b2'), random_ref('b3')
    add_stock([
        (batch1, sku, 100, '2011-01-02'),
        (batch2, sku, 100, '2011-01-01'),
        (batch3, othersku, 100, None),
    ])
    data = {
        'orderid': random_ref('o'),
        'sku': sku,
        'qty': 3,
    }
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 201
    assert r.json()['batchid'] == batch2
----
====

Everyone solves these problems in different ways, but you're going
to need some way of spinning up Flask, possibly in a container, and
also talking to a postgres database.  If you want to see how we did
it, check out <<appendix_project_structure>>.


=== The straightforward implementation

Implementing things in the most obvious way, you might get something like this:


[[first_cut_flask_app]]
.First cut flask app (flask_app.py)
====
[source,python]
[role="non-head"]
----
orm.start_mappers()
get_session = sessionmaker(bind=create_engine(config.get_postgres_uri()))
app = Flask(__name__)

@app.route("/allocate", methods=['POST'])
def allocate_endpoint():
    session = get_session()
    batches = repository.BatchRepository(session).list()
    line = model.OrderLine(
        request.json['orderid'],
        request.json['sku'],
        request.json['qty'],
    )

    batchid = model.allocate(line, batches)

    return jsonify({'batchid': batchid}), 201
----
====


So far so good.  No need for too much more of your "architecture astronaut"
nonsense, Bob and Harry, you may be thinking.

But hang on a minute -- there's no commit.  We're not actually saving our
allocation to the database. Now we need a second test, either one that will
inspect the database state after (not very black-boxey), or maybe one that
checks we can't allocate a second line if a first should have already depleted
the batch:

[[second_api_test]]
.Test allocations are persisted (test_api.py)
====
[source,python]
[role="non-head"]
----
def test_allocations_are_persisted(add_stock):
    sku = random_ref('s1')
    batch1, batch2 = random_ref('b1'), random_ref('b2')
    order1, order2 = random_ref('o1'), random_ref('o2')
    add_stock([
        (batch1, sku, 10, '2011-01-01'),
        (batch2, sku, 10, '2011-01-02'),
    ])
    line1 = {'orderid': order1, 'sku': sku, 'qty': 10}
    line2 = {'orderid': order2, 'sku': sku, 'qty': 10}
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=line1)
    assert r.status_code == 201
    assert r.json()['batchid'] == batch1
    r = requests.post(f'{url}/allocate', json=line2)
    assert r.status_code == 201
    assert r.json()['batchid'] == batch2
----
====

Not quite so lovely, but that will force us to get a commit in.



=== Error conditions that require database checks

Let's add a bit of error-handling shall we?  What if the domain raises an
error, for a sku that's out of stock?  Or what about a sku that doesn't even
exist, that's not something the domain even knows about, nor should it.  It's
more of a sanity-check that we should implement at the database layer, before
we even invoke the domain service.

Now we're looking at two more end-to-end tests:

[[test_error_cases]]
.Yet more tests at the e2e layer...  (test_api.py)
====
[source,python]
[role="non-head"]
----
@pytest.mark.usefixtures('restart_api')
def test_400_message_for_out_of_stock(add_stock):
    sku, batch, order = random_ref('s'), random_ref('b'), random_ref('o')
    add_stock([
        (batch, sku, 10, '2011-01-01'),
    ])
    data = {'orderid': order, 'sku': sku, 'qty': 20}
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 400
    assert r.json()['message'] == f'Out of stock for sku {sku}'


@pytest.mark.usefixtures('restart_api')
def test_400_message_for_invalid_sku():
    sku, order = random_ref('s'), random_ref('o')
    data = {'orderid': order, 'sku': sku, 'qty': 20}
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 400
    assert r.json()['message'] == f'Invalid sku {sku}'
----
====

And, sure we can implement that in the flask app too:

[[flask_error_handling]]
.First cut flask app (flask_app.py)
====
[source,python]
[role="non-head"]
----
def is_valid_sku(sku, batches):
    return sku in {b.sku for b in batches}

@app.route("/allocate", methods=['POST'])
def allocate_endpoint():
    session = get_session()
    batches = repository.BatchRepository(session).list()
    line = model.OrderLine(
        request.json['orderid'],
        request.json['sku'],
        request.json['qty'],
    )

    if not is_valid_sku(line.sku, batches):
        return jsonify({'message': f'Invalid sku {line.sku}'}), 400

    try:
        batchid = model.allocate(line, batches)
    except model.OutOfStock as e:
        return jsonify({'message': str(e)}), 400

    session.commit()
    return jsonify({'batchid': batchid}), 201
----
====

But our flask app is starting to look a bit unwieldy.  And our number of
E2E tests is starting to get out of control, and soon we'll end up with an
inverted test pyramid (or "ice cream cone model" as Bob likes to call it).


=== Introducing a service layer, and using FakeRepository to unit test it

If we look at what our flask app is doing, there's quite a lot of what we
might call "orchestration" -- fetching stuff out of our repository, validating
our input against database state, handling errors, and committing in the
happy path.  Most of these things aren't anything to do with having a
web API endpoint (you'd need them if you were building a CLI for example, see
<<appendix_csvs>>), and they're not really things that need to be tested by
end-to-end tests.

It often makes sense to split out a "service layer", sometimes called
"orchestration layer' or "use case layer".  And our `FakeRepository`
will come in very useful for testing it with nice, fast unit tests:

[[first_services_tests]]
.Unit testing with fakes at the services layer (test_services.py)
====
[source,python]
[role="non-head"]
----
def test_returns_allocation():
    line = model.OrderLine('o1', 'sku1', 10)
    batch = model.Batch('b1', 'sku1', 100, eta=None)
    repo = FakeRepository([batch])

    result = services.allocate(line, repo, FakeSession())
    assert result == 'b1'


def test_error_for_invalid_sku():
    line = model.OrderLine('o1', 'nonexistentsku', 10)
    batch = model.Batch('b1', 'actualsku', 100, eta=None)
    repo = FakeRepository([batch])

    with pytest.raises(services.InvalidSku) as ex:
        services.allocate(line, repo, FakeSession())

    assert 'Invalid sku nonexistentsku' in str(ex)
----
====


Alongside `FakeRepository`, we also build `FakeSession` (that latter is a bit
of a temporary solution though.  We'll get rid of it and make things _even nicer_
in the next chapter, <<chapter_04_uow>>)


[[fake_repo]]
.Our fake repository (test_services.py)
====
[source,python]
----
class FakeRepository(set):

    def get(self, reference):
        return next(x for x in self if x.reference == reference)

    def list(self):
        return list(self)


class FakeSession():
    committed = False

    def commit(self):
        self.committed = True
----
====

And that lets us migrate a third test from the E2E layer, the one that
checks that we do a commit:



[[second_services_test]]
.A second test at the service layer (test_services.py)
====
[source,python]
[role="non-head"]
----
def test_commits():
    line = model.OrderLine('o1', 'sku1', 10)
    batch = model.Batch('b1', 'sku1', 100, eta=None)
    repo = FakeRepository([batch])
    session = FakeSession()

    services.allocate(line, repo, session)
    assert session.committed is True
----
====

We'll get to a service function that looks something like <<service_function>>:

[[service_function]]
.Basic allocation service (services.py)
====
[source,python]
[role="non-head"]
----
class InvalidSku(Exception):
    pass


def is_valid_sku(sku, batches):  #<2>
    return sku in {b.sku for b in batches}

def allocate(line: OrderLine, repo: BatchRepository, session) -> str:
    batches = repo.list()  #<1>
    if not is_valid_sku(line.sku, batches):  #<2>
        raise InvalidSku(f'Invalid sku {line.sku}')
    batch = model.allocate(line, batches)  #<3>
    session.commit()  #<4>
    return batch
----
====

Typical service-layer functions have similar steps:

<1> We fetch some objects from the repository

<2> We make some checks or assertions about the request against
    the current state of the world

<3> We call a domain service

<4> And if all is well, we save/update any state we've changed.

That last step is a little unsatisfactory at the moment, our services
layer is tightly coupled to our database layer, but we'll improve on
that in the next chapter.

But the essentials of the services layer are there, and our flask
app now looks a lot cleaner, <<flask_app_using_service_layer>>:


[[flask_app_using_service_layer]]
.Flask app delegating to service layer (flask_app.py)
====
[source,python]
[role="non-head"]
----
@app.route("/allocate", methods=['POST'])
def allocate_endpoint():
    session = get_session()  #<1>
    repo = repository.BatchRepository(session)  #<1>
    line = model.OrderLine(
        request.json['orderid'],  #<2>
        request.json['sku'],  #<2>
        request.json['qty'],  #<2>
    )
    try:
        batchid = services.allocate(line, repo, session)  #<2>
    except (model.OutOfStock, services.InvalidSku) as e:
        return jsonify({'message': str(e)}), 400  <3>

    return jsonify({'batchid': batchid}), 201  <3>
----
====

We see that the responsibilities of the flask app are much more minimal, and
more focused on just the web stuff:

<1> We instantiate a database session and some repository objects.
<2> We extract the user's commands from the web request and pass them
    to a domain service.
<3> And we return some JSON responses with the appropriate status codes

The responsibilities of the flask app are just standard web stuff: per-request
session management, parsing information out of POST parameters, response status
codes and JSON.  All the orchestration logic is in the use case / service layer,
and the domain logic stays in the domain.


Finally we can confidently strip down our E2E tests to just two, one for
the happy path and one for the unhappy path:


[[fewer_e2e_tests]]
.E2E tests now only happy + unhappy paths (test_api.py)
====
[source,python]
[role="non-head"]
----
import uuid
import pytest
import requests

import config

def random_ref(prefix):
    return prefix + '-' + uuid.uuid4().hex[:10]

@pytest.mark.usefixtures('restart_api')
def test_happy_path_returns_201_and_allocated_batch(add_stock):
    sku, othersku = random_ref('s1'), random_ref('s2')
    batch1, batch2, batch3 = random_ref('b1'), random_ref('b2'), random_ref('b3')
    add_stock([
        (batch1, sku, 100, '2011-01-02'),
        (batch2, sku, 100, '2011-01-01'),
        (batch3, othersku, 100, None),
    ])
    data = {
        'orderid': random_ref('o'),
        'sku': sku,
        'qty': 3,
    }
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 201
    assert r.json()['batchid'] == batch2


@pytest.mark.usefixtures('restart_api')
def test_unhappy_path_returns_400_and_error_message():
    sku, order = random_ref('s'), random_ref('o')
    data = {'orderid': order, 'sku': sku, 'qty': 20}
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 400
    assert r.json()['message'] == f'Invalid sku {sku}'

----
====

We've successfully split our tests into two broad categories: tests about web
stuff, which we implement end-to-end; and tests about orchestration stuff, which
we can test against the service layer in memory.

=== How is our test pyramid looking?

Let's see what this move to using a service layer, with its own service-layer tests,
does to our test pyramid:

[[test_pyramid]]
.Counting different types of test
====
[source,sh]
[role="skip"]
----
ðŸ‘‰  grep -c test_ test_*.py
test_allocate.py:4
test_batches.py:8
test_services.py:3

test_orm.py:6
test_repository.py:2

test_api.py:4
----
====

//TODO: test listing this too?

Not bad!  15 unit tests, 8 integration tests, and just 2 end-to-end tests.  That's
a healthy-looking test pyramid.



=== Should domain layer tests move to the service layer?

We could take this a step further. Since we can test the our software against
the service layer, we don't really need tests for the domain model any more.
Instead, we could rewrite all of the domain-level tests from chapter one in
terms of the service layer.

.Rewriting a domain test at the service layer (test_services.py)
====
[source,python]
[role="skip"]
----
def test_prefers_warehouse_batches_to_shipments():
    warehouse_batch = Batch('wh-batch', 'sku1', 100, eta=None)
    shipment_batch = Batch('sh-batch', 'sku1', 100, eta=tomorrow)
    repo = FakeRepository([warehouse_batch, shipment_batch])
    session = FakeSession()

    line = OrderLine('oref', 'sku1', 10)

    services.allocate(line, repo, session)

    assert warehouse_batch.available_quantity == 90
----
====

Why would we want to do that?

Tests are supposed to help us change our system fearlessly, but very often
we see teams writing too many tests against their domain model. This causes
problems when they come to change their codebase, and find that they need to
update tens or even hundreds of unit tests.

This makes sense if you stop to think about the purpose of automated tests. We
use tests to enforce that some property of the system doesn't change while we're
working. We use tests to check that the API continues to return 200, that the
database session continues to commit, and that orders are still being allocated.

If we accidentally change one of those behaviours, our tests will break. The
flip side, though, is that if we want to change the design of our code, any
tests relying directly on that code will also fail.

Every line of code that we put in a test is like a blob of glue, holding the
system in a particular shape.

As we get further into the book, we'll see how the service layer forms an API
for our system that we can drive in multiple ways. Testing against this API
reduces the amount of code that we need to change when we refactor our domain
model. If we restrict ourselves to only testing against the service layer,
we won't have any tests that directly interact with "private" methods or
attributes on our model objects, which leaves us more free to refactor them.

==== When should I write tests against the domain?

You might be asking yourself "should I rewrite all my unit tests, then? Is it
wrong to write tests against the domain model?". To answer the question, it's
important to understand the trade-off between coupling and design feedback.

[insert diagram, pls]

| Low feedback                                                  High feedback |
| Low barrier to change                                 High barrier to change|
| High system coverage                                       Focused coverage |
| <---------                                                         ------>  |
| API tests                  service-layer tests                 domain tests |

Extreme Programming (XP) exhorts us to "listen to the code". When we're writing
tests, we might find that the code is hard to use, or notice a code smell. This
is a trigger for us to refactor, and reconsider our design.

We only get that feedback, though, when we're working closely with the target
code. A test for the HTTP API tells us nothing about the fine-grained design of
our objects, because it sits at a much higher level of abstraction.

On the other hand, we can rewrite our entire application and, so long as we
don't change the URLs or request formats, our http tests will continue to pass.
This gives us confidence that large-scale changes, like changing the DB schema,
haven't broken our code.

At the other end of the spectrum, the tests we wrote in chapter 1 helped us to
flesh out our understanding of the objects we need. The tests guided us to a
design that makes sense and reads in the domain language. When our tests read
in the domain language, we feel comfortable that our code matches our intuition
about the problem we're trying to solve.

Because the tests are written in the domain language, they act as living
documentation for our model. A new team member can read these tests to quickly
understand how the system works, and how the core concepts interrelate.

We often "sketch" new behaviours by writing tests at this level to see how the
code might look.

When we want to improve the design of the code, though, we will need to replace
or delete these tests, because they are tightly coupled to a particular
implementation.

==== Low and High Gear

Most of the time, when we are adding a new feature, or fixing a bug, we don't
need to make extensive changes to the domain model. In these cases, we prefer
to write tests against services for the lower-coupling and high-coverage.

For example, when writing an `add_stock` function, or a `cancel_order` feature,
we can work more quickly and with less coupling by writing tests against the
service layer.

When starting out a new project, or when we hit a particularly gnarly problem,
we will drop back down to writing tests against the domain model, so that we
get better feedback and executable documentation of our intent.

The metaphor we use is that of shifting gears. When starting off a journey, the
car needs to be in a low gear so that it can overcome inertia. Once we're off
and running, we can go faster and more efficiently by changing into a high gear
but if we suddenly encounter a steep hill, or we're forced to slow down by a
hazard, we again drop down to a low gear until we can pick up speed again.

[sidebar]

Rules of thumb:

* Write one black-box test per-feature to demonstrate that the feature exists
  and is working. This might be written against an HTTP api. These tests cover
  an entire feature at a time.
* Write the bulk of the tests for your system against the service layer. This
  offers a good trade-off between coverage, run-time, and efficiency. These
  tests tend to cover one code path of a feature and use fakes for IO.
* Maintain a small core of tests written against your domain model. These tests
  have highly-focused coverage, and are more brittle, but have the highest
  feedback. Don't be afraid to delete these tests if the functionality is
  later covered by tests at the service layer.


=== Fully Decoupling the service layer from the domain

We still have some direct dependencies on the domain in our service-layer
tests, because we use domain objects to set up our test data and to invoke
our service-layer functions.

To have a service layer that's fully decoupled from the domain, we need to
rewrite its API to work in terms of primitives.

Our service layer currently takes an `OrderLine` domain object.

[[service_domain]]
.service takes a domain object: (services.py)
====
[source,python]
[role="skip"]
----
def allocate(line: OrderLine, repo: BatchRepository, session) -> str:
----
====

How would it look if its parameters were all primitive types?

[[service_takes_primitives]]
.service takes strings and ints (services.py)
====
[source,python]
----
def allocate(
        orderid: str, sku: str, qty: int, repo: BatchRepository, session
) -> str:
----
====


We rewrite the tests in those terms as well:


[[tests_call_with_primitives]]
.Tests now use primitives in function call (test_services.py)
====
[source,python]
[role="non-head"]
----
def test_returns_allocation():
    batch = model.Batch('b1', 'sku1', 100, eta=None)
    repo = FakeRepository([batch])

    result = services.allocate('o1', 'sku1', 10, repo, FakeSession())
    assert result == 'b1'
----
====

But our tests still depend on the domain, because we still manually
instantiate `Batch` objects.  So if we decide to massively refactor
how our Batch model works, we'll have to change a bunch of tests.


==== Mitigation: keep all domain dependencies in fixture functions

We could at least abstract that out to a helper function or a fixture
in our tests.  Here's one way you could do that, adding a factory
function on `FakeRepository`:


[[services_factory_function]]
.Factory functions for fixtures are one possibility (test_services.py)
====
[source,python]
[role="skip"]
----
class FakeRepository(set):

    @staticmethod
    def for_batch(ref, sku, qty, eta=None):
        return FakeRepository([
            model.Batch(ref, sku, qty, eta),
        ])

    #...


def test_returns_allocation():
    repo = FakeRepository.for_batch('b1', 'sku1', 100, eta=None)
    result = services.allocate('o1', 'sku1', 10, repo, FakeSession())
    assert result == 'b1'
----
====

At least that would move all of our tests' dependencies on the domain
into one place.


==== Adding a missing service

We could go one step further though.  If we had a service to add stock,
then we could use that, and make our service-layer tests fully expressed
in terms of the service layer's official use cases, removing all dependencies
on the domain:


[[test_add_batch]]
.Test for new add_batch service (test_services.py)
====
[source,python]
----
def test_add_batch():
    repo, session = FakeRepository(), FakeSession()
    services.add_batch('b1', 'sku1', 100, None, repo, session)
    assert repo.get('b1') is not None
    assert session.committed
----
====


And the implementation is just two lines

[[add_batch_service]]
.A new service for add_batch (services.py)
====
[source,python]
----
def add_batch(
        ref: str, sku: str, qty: int, eta: Optional[date],
        repo: BatchRepository, session,
):
    repo.add(model.Batch(ref, sku, qty, eta))
    session.commit()
----
====

NOTE: Should you write a new service just because it would help remove
    dependencies from your tests?  Probably not.  But in this case, we
    probably would need an add_batch service one day anyway.  In general
    though, if you find yourself needing to do domain-layer stuff directly
    in your service-layer tests, it may be an indication that your service
    layer is incomplete.
    

That now allows us to rewrite _all_ of our service-layer tests purely 
in terms of the services themselves, using only primitives, and without
any dependencies on the model.


[[services_tests_all_services]]
.Services tests now only use services (test_services.py)
====
[source,python]
----
def test_allocate_returns_allocation():
    repo, session = FakeRepository(), FakeSession()
    services.add_batch('b1', 'sku1', 100, None, repo, session)
    result = services.allocate('o1', 'sku1', 10, repo, session)
    assert result == 'b1'


def test_allocate_errors_for_invalid_sku():
    repo, session = FakeRepository(), FakeSession()
    services.add_batch('b1', 'actualsku', 100, None, repo, session)

    with pytest.raises(services.InvalidSku) as ex:
        services.allocate('o1', 'nonexistentsku', 10, repo, FakeSession())

    assert 'Invalid sku nonexistentsku' in str(ex)
----
====


This is a really nice place to be in.  Our service-layer tests only depend on
the services layer itself, leaving us completely free to refactor the model as
we see fit.

=== Carrying the improvement through to the E2E tests

In the same way that adding `add_batch` helped decouple our services-layer
tests from the model, adding an API endpoint to add a batch would remove
the need for the ugly `add_stock` fixture, and our E2E tests can be free
of those hardcoded SQL queries and the direct dependency on the database.

The service function means adding the endpoint is very easy, just a little
json-wrangling and a single function call:


[[api_for_add_batch]]
.API for adding a batch (flask_app.py)
====
[source,python]
----
@app.route("/add_batch", methods=['POST'])
def add_batch():
    session = get_session()
    repo = repository.BatchRepository(session)
    eta = request.json['eta']
    if eta is not None:
        eta = datetime.fromisoformat(eta).date()
    services.add_batch(
        request.json['ref'], request.json['sku'], request.json['qty'], eta,
        repo, session
    )
    return 'OK', 201
----
====


And our hardcoded SQL queries from _conftest.py_ get replaced with some
API calls, meaning the API tests have no dependencies other than the API,
which is also very nice:

[[api_tests_with_no_sql]]
.API tests can now add their own batches (test_api.py)
====
[source,python]
----
def post_to_add_batch(ref, sku, qty, eta):
    url = config.get_api_url()
    r = requests.post(
        f'{url}/add_batch',
        json={'ref': ref, 'sku': sku, 'qty': qty, 'eta': eta}
    )
    assert r.status_code == 201


@pytest.mark.usefixtures('postgres_db')
@pytest.mark.usefixtures('restart_api')
def test_happy_path_returns_201_and_allocated_batch():
    sku, othersku = random_ref('s1'), random_ref('s2')
    batch1, batch2, batch3 = random_ref('b1'), random_ref('b2'), random_ref('b3')
    post_to_add_batch(batch1, sku, 100, '2011-01-02')
    post_to_add_batch(batch2, sku, 100, '2011-01-01')
    post_to_add_batch(batch3, othersku, 100, None)
    data = {
        'orderid': random_ref('o'),
        'sku': sku,
        'qty': 3,
    }
    url = config.get_api_url()
    r = requests.post(f'{url}/allocate', json=data)
    assert r.status_code == 201
    assert r.json()['batchid'] == batch2
----
====


=== Wrap-up


Adding the service layer has really bought us quite a lot:

* Our flask API endpoints become very thin and easy to write:  their
  only responsibility is doing "web stuff", things like parsing JSON
  and producing the right HTTP codes for happy or unhappy cases.

* We've defined a clear API for our domain, a set of use cases or
  entrypoints that can be used by any adapter without needing to know anything
  about our domain model classes--whether that's an API, a CLI (see
  <<appendix_csvs>>), or the tests! They're an adapter for our domain too.
    
* We can write tests in "high gear" using the service layer, leaving us
  free to refactor the domain model in any way we see fit.  As long as
  we can still deliver the same use cases, we can experiment with new 
  designs without needing to rewrite a load of tests.

* And our "test pyramid" is looking good -- the bulk of our tests
  are fast/unit tests, with just the bare minimum of E2E and integration
  tests.
  
There's still a bit of awkwardness we'd like to get rid of. The service
layer is tightly coupled to a `session` object.  In the next chapter, we'll
introduce one more pattern that works closely with _Repository_ and _Service
Layer_, the _Unit of Work_ pattern, and everything will be absolutely lovely.
You'll see!


.API and Service Layer recap
*****************************************************************
Apply the DIP to Flask as well--your API is a port::
    Your web framework should depend on your domain model and
    service layers, not the other way around.

The service layer allows your external integrations to be very thin::
    The service layer does all the work of fetching things from
    your repository, doing data validation, creating domain objects,
    invoking domain methods and services, and raising the appropriate
    errors.  This way, whether your external interface is only a very
    thin wrapper, whether it's a Flask API or a CLI that uses CSVs
    (see <<appendix_csvs>>)

*****************************************************************
